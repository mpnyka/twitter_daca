{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet1 = []\n",
    "tweet2 = []\n",
    "tweet3 = []\n",
    "tweet4 = []\n",
    "tweets = []\n",
    "with open('DACA_streaming_M5_2.json', 'r') as f:\n",
    "    for line in f:\n",
    "        tweet1.append(json.loads(line))\n",
    "\n",
    "        \n",
    "for t in tweet1:\n",
    "    if t[\"lang\"] == \"en\":\n",
    "        if t[\"retweeted\"] == False:\n",
    "            if t[\"truncated\"] == True:\n",
    "                tweets.append(t[\"extended_tweet\"][\"full_text\"])\n",
    "            else:\n",
    "                tweets.append(t[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(tweet3)\n",
    "print len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('DACA_streaming_M5_2c.json', 'r') as f:\n",
    "    for line in f:\n",
    "        tweet2.append(json.loads(line))\n",
    "\n",
    "for t in tweet2:\n",
    "    if t[\"lang\"] == \"en\":\n",
    "        if t[\"retweeted\"] == False:\n",
    "            if t[\"truncated\"] == True:\n",
    "                tweets.append(t[\"extended_tweet\"][\"full_text\"])\n",
    "            else:\n",
    "                tweets.append(t[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(tweet2)\n",
    "print len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "i=0\n",
    "tweet3 = []\n",
    "with open('DACA_streaming_M5_3.json', 'r') as f:\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        if i < 100000:\n",
    "            tweet3.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "i=0\n",
    "tweet3_2 = []\n",
    "with open('DACA_streaming_M5_3.json', 'r') as f:\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        if i > 100000 and i <200000:\n",
    "            tweet3_2.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "i=0\n",
    "## they in inverse order...\n",
    "with open('DACA_streaming_M5_3.json', 'r') as f:\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        if i > 200000 and i <300000:\n",
    "            tweet3_2.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "i=0\n",
    "tweet3_3 = []\n",
    "with open('DACA_streaming_M5_3.json', 'r') as f:\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        print i\n",
    "        if i > 300000 and i <500000:\n",
    "            tweet3_3.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "i=0\n",
    "tweet3_5 = []\n",
    "with open('DACA_streaming_M5_3.json', 'r') as f:\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        print i\n",
    "        if i > 500000 and i <700000:\n",
    "            tweet3_5.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "i=0\n",
    "tweet3_6 = []\n",
    "with open('DACA_streaming_M5_3.json', 'r') as f:\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        if i > 700000 and i <900000:\n",
    "            tweet3_6.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "i=0\n",
    "tweet3_5 = []\n",
    "with open('DACA_streaming_M5_3.json', 'r') as f:\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        if i > 900000 and i <1100000:\n",
    "            tweet3_5.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(tweet3)\n",
    "print len(tweet3_2)\n",
    "print len(tweet3_3)\n",
    "print len(tweet3_4) ## last tweets\n",
    "\n",
    "\n",
    "#1079211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tweet3, open(\"tweets_M5_3_1.pkl\", \"wb\"))\n",
    "pickle.dump(tweet3_2, open(\"tweets_M5_3_2.pkl\", \"wb\"))\n",
    "pickle.dump(tweet3_3, open(\"tweets_M5_3_3.pkl\", \"wb\"))\n",
    "pickle.dump(tweet3_4, open(\"tweets_M5_3_4.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for t in tweet3:\n",
    "    if t[\"lang\"] == \"en\":\n",
    "        if t[\"retweeted\"] == False:\n",
    "            if t[\"truncated\"] == True:\n",
    "                tweets.append(t[\"extended_tweet\"][\"full_text\"])\n",
    "            else:\n",
    "                tweets.append(t[\"text\"])\n",
    "len(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('DACA_streaming_M23.json', 'r') as f:\n",
    "    for line in f:\n",
    "        tweet4.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in tweet4:\n",
    "    if t[\"lang\"] == \"en\":\n",
    "        if t[\"retweeted\"] == False:\n",
    "            if t[\"truncated\"] == True:\n",
    "                tweets.append(t[\"extended_tweet\"][\"full_text\"])\n",
    "            else:\n",
    "                tweets.append(t[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(tweet1)\n",
    "print len(tweet2)\n",
    "#print len(tweet3)\n",
    "print len(tweet4)\n",
    "print len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tweets, open(\"tweets_M5_2.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_noRT = []\n",
    "t_RT = []\n",
    "\n",
    "\n",
    "\n",
    "for t in tweets:\n",
    "    if \"RT @\" in t:\n",
    "         t_RT.append(t)\n",
    "    if \"RT @\" not in t:\n",
    "        t_noRT.append(t)\n",
    "        \n",
    "print len(t_noRT)\n",
    "print len(t_RT)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tweets_noRT=[]\n",
    "for t in tweets:\n",
    "    matches = re.findall(r'(?<=RT\\s)@\\S+:', t)\n",
    "    if len(matches) == 1:\n",
    "        t = re.sub(\"RT \"+ matches[0] + \" \", '', t)\n",
    "        tweets_noRT.append(t)\n",
    "    else:\n",
    "        tweets_noRT.append(t)\n",
    "tweets_noURL =[]\n",
    "for t in tweets_noRT:\n",
    "    t = re.sub(r\"http\\S+\", \"\", t)\n",
    "    tweets_noURL.append(t)\n",
    "\n",
    "print len(tweets)\n",
    "print len(tweets_noRT)\n",
    "print len(tweets_noURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display (tweets_noURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f8(seq): # Dave Kirby\n",
    "    # Order preserving\n",
    "    seen = set()\n",
    "    return [x for x in seq if x not in seen and not seen.add(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_uniq = f8(tweets_noRT)\n",
    "print len(tweets_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pickle\n",
    "from socialsent import util\n",
    "\n",
    "tweets_uniq = util.load_pickle(\"tweets_M_uniq.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_noAT = []\n",
    "for t in tweets_uniq:\n",
    "    t = re.sub(r\"@\\S+\", \"\", t)\n",
    "    tweets_noAT.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(tweets_uniq)\n",
    "print len(tweets_noAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_ascii = []\n",
    "for t in tweets_noAT:\n",
    "    tweets_ascii.append(t.encode('ascii', 'ignore'))\n",
    "df = pandas.DataFrame(tweets_ascii, columns=[\"colummn\"])\n",
    "df.to_csv('tweets_M.csv', index=False)\n",
    "pickle.dump(tweets_ascii, open(\"tweets_M.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stoplist = set('all full as march 5th five 5 call inside outside today yesterday tomorrow by does not do isnt wasnt arent werent been got for a an of the and to in on off rt according at this that it or its be many very several but is are am just was were get have has had his him her she he they them i our their me my you yours your with while into onto between since from n more less how where who when then before after'.split())\n",
    "stopwords= set(line.strip() for line in open('stopwords.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "#stoplist = set('all full as march 5th five 5 call inside outside today yesterday tomorrow by does not do isnt wasnt arent werent been got for a an of the and to in on off rt according at this that it or its be many very several but is are am just was were get have has had his him her she he they them i our their me my you yours your with while into onto between since from n more less how where who when then before after'.split())\n",
    "stopwords= set(line.strip() for line in open('stopwords.txt'))\n",
    "texts = [[word for word in tweet.lower().split() if word not in stopwords]\n",
    "         for tweet in tweets_ascii]\n",
    "print texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "     for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 10]\n",
    "         for text in texts]\n",
    "\n",
    "print texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "pickle.dump(texts, open(\"tweets_M_token.pkl\", \"wb\"))\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "pickle.dump(dictionary, open(\"twitter_dict.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
