{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import socialsent\n",
    "from socialsent import util\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "from socialsent.representations import ppmigen, cooccurgen, makelowdim, sparse_io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_gen(file, gdict):\n",
    "    for i, line in enumerate(open(file)):\n",
    "        info = line.split(\"\\t\")\n",
    "        comment = info[-1]\n",
    "        for word in comment.split():\n",
    "            word = word.lower()\n",
    "#            if word != \"<EOS>\" and word in gdict.token2id:\n",
    "            if word in gdict.token2id:\n",
    "                yield word\n",
    "        if i % 10000 == 0:\n",
    "            print \"Processed line\", i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdict = util.load_pickle(\"twitter_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(0 unique tokens: [])\n"
     ]
    }
   ],
   "source": [
    "print gdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdict.filter_extremes(no_above=0.5, no_below=50)\n",
    "gdict.compactify()\n",
    "outpath = \"data/\"\n",
    "util.write_pickle(gdict.token2id, outpath + \"index.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "from socialsent import util\n",
    "\n",
    "def run(word_gen, index, window_size, out_file):\n",
    "    context = []\n",
    "    pair_counts = Counter()\n",
    "    for word in word_gen:\n",
    "        context.append(index[word])\n",
    "        if len(context) > window_size * 2 + 1:\n",
    "            context.pop(0)\n",
    "        pair_counts = _process_context(context, pair_counts, window_size)\n",
    "    import pyximport\n",
    "    pyximport.install(setup_args={\"include_dirs\": np.get_include()})\n",
    "    sparse_io.export_mat_from_dict(pair_counts, out_file)\n",
    "\n",
    "def _process_context(context, pair_counts, window_size):\n",
    "    if len(context) < window_size + 1:\n",
    "        return pair_counts\n",
    "    target = context[window_size]\n",
    "    indices = range(0, window_size)\n",
    "    indices.extend(range(window_size + 1, 2 * window_size + 1))\n",
    "    for i in indices:\n",
    "        if i >= len(context):\n",
    "            break\n",
    "        pair_counts[(target, context[i])] += 1\n",
    "    return pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed line 0\n",
      "Processed line 10000\n",
      "Processed line 20000\n",
      "Processed line 30000\n",
      "Processed line 40000\n",
      "Processed line 50000\n",
      "Processed line 60000\n",
      "Processed line 70000\n",
      "Processed line 80000\n",
      "Processed line 90000\n",
      "Processed line 100000\n",
      "Processed line 110000\n",
      "Processed line 120000\n",
      "Processed line 130000\n",
      "Processed line 140000\n",
      "Processed line 150000\n",
      "Processed line 160000\n",
      "Processed line 170000\n",
      "Processed line 180000\n",
      "Processed line 190000\n",
      "Processed line 200000\n",
      "Processed line 210000\n",
      "Processed line 220000\n",
      "Processed line 230000\n",
      "Processed line 240000\n",
      "Processed line 250000\n",
      "Processed line 260000\n",
      "Processed line 270000\n",
      "Processed line 280000\n",
      "Processed line 290000\n",
      "Processed line 300000\n",
      "Processed line 310000\n",
      "Processed line 320000\n",
      "Processed line 330000\n",
      "Processed line 340000\n",
      "Processed line 350000\n",
      "Processed line 360000\n",
      "Processed line 370000\n",
      "Processed line 380000\n",
      "Processed line 390000\n",
      "Processed line 400000\n",
      "Processed line 410000\n",
      "Processed line 420000\n",
      "Processed line 430000\n",
      "Processed line 440000\n",
      "Processed line 450000\n",
      "Processed line 460000\n",
      "Processed line 470000\n",
      "Processed line 480000\n",
      "Processed line 490000\n",
      "Processed line 500000\n",
      "Processed line 510000\n",
      "Processed line 520000\n",
      "Processed line 530000\n",
      "Processed line 540000\n",
      "Processed line 550000\n",
      "Processed line 560000\n",
      "Processed line 570000\n",
      "Processed line 580000\n",
      "Processed line 590000\n",
      "Processed line 600000\n",
      "Processed line 610000\n",
      "Processed line 620000\n"
     ]
    }
   ],
   "source": [
    "run(word_gen(\"tweets_M.pkl\", gdict), gdict.token2id, 4,  outpath + \"counts.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from socialsent.representations.representation_factory import create_representation\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def make_ppmi_mat(old_mat, row_probs, col_probs, smooth, neg=1, normalize=False):\n",
    "    prob_norm = old_mat.sum() + (old_mat.shape[0] * old_mat.shape[1]) * smooth\n",
    "    old_mat = old_mat.tocoo()\n",
    "    row_d = old_mat.row\n",
    "    col_d = old_mat.col\n",
    "    data_d = old_mat.data\n",
    "    neg = np.log(neg)\n",
    "    for i in xrange(len(old_mat.data)):\n",
    "        if data_d[i] == 0.0:\n",
    "            continue\n",
    "        joint_prob = (data_d[i] + smooth) / prob_norm\n",
    "        denom = row_probs[row_d[i], 0] * col_probs[0, col_d[i]]\n",
    "        if denom == 0.0:\n",
    "            data_d[i] = 0\n",
    "            continue\n",
    "        data_d[i] = np.log(joint_prob /  denom)\n",
    "        data_d[i] = max(data_d[i] - neg, 0)\n",
    "        if normalize:\n",
    "            data_d[i] /= -1*np.log(joint_prob)\n",
    "    return coo_matrix((data_d, (row_d, col_d)))\n",
    "\n",
    "def ppmi_run(count_path, out_path, smooth=0, cds=True, normalize=False, neg=1):\n",
    "    counts = create_representation(\"Explicit\", count_path, normalize=False)\n",
    "    old_mat = counts.m\n",
    "    index = counts.wi\n",
    "    smooth = old_mat.sum() * smooth\n",
    "\n",
    "    # getting marginal probs\n",
    "    row_probs = old_mat.sum(1) + smooth\n",
    "    col_probs = old_mat.sum(0) + smooth\n",
    "    if cds:\n",
    "        col_probs = np.power(col_probs, 0.75)\n",
    "    row_probs = row_probs / row_probs.sum()\n",
    "    col_probs = col_probs / col_probs.sum()\n",
    "\n",
    "    # building PPMI matrix\n",
    "    ppmi_mat = make_ppmi_mat(old_mat, row_probs, col_probs, smooth, neg=neg, normalize=normalize)\n",
    "    import pyximport\n",
    "    pyximport.install(setup_args={\"include_dirs\": np.get_include()})\n",
    "    sparse_io.export_mat_eff(ppmi_mat.row, ppmi_mat.col, ppmi_mat.data, out_path + \".bin\")\n",
    "    util.write_pickle(index, out_path + \"-index.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find local index. Attempting to load directory wide index...\n"
     ]
    }
   ],
   "source": [
    "ppmi_run(outpath + \"counts.bin\", outpath + \"ppmi\", cds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makelowdim.run(outpath + \"ppmi.bin\", outpath + \"vecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/counts.bin'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
